---
title: "Stan tutorial"
author: "Steve Simon"
date: "December 1, 2017"
output: html_document
---

```{r render-this-file, echo=FALSE, eval=FALSE}
# To save the output in the proper subdirectory,
# save this file and run the two commands below.
library(rmarkdown)
render("~/stan-tutorial/src/stan.Rmd",
  output_dir="~/stan-tutorial/results")
```

## Introduction

This handout was created using RMarkdown. You can get this handout and all of the code used to produce this handout at

https://github.com/pmean/stan-tutorial.

This program shows a few features of Stan, a program to conduct Bayesian data analyses. A detailed overview of the design of Stan appears in

Carpenter B et al (2017) Stan: A Probabilistic Programming Language. Journal of Statistical Software 70(1). Available at https://www.jstatsoft.org/article/view/v076i01. Hereafter, I will refer to this paper as just "Carpenter."

You can find other resources for Stan at http://mc-stan.org/.

## Installation details

You can run Stan as a stand-alone program, or you can run it from inside Python or R. I will show how to run a few simple Stan programs from within R using the RStan library.

As described in footnote 2 on page 3 of Carpenter, Stan takes code written in its own language, translates it into C++, compiles it, runs it, and saves the output to an object in R. This means that installation is a lot trickier than for most R libraries.

In particular, you need a toolchain (a set of programming tools that help with the compiling and running of C++ code within R). The folks at Stan recommend RTools for Windows users. For the Mac, they recommend that you use "an official Xcode release from Apple."  Linux usually has all the pieces you need ("use your package manager to install build-essential and a recent version of either g++ or clang++. The package libssl-dev (up to version 1.0; version 1.1 brakes package PKI) is required as well."). 

I am totally clueless about the toolchain suggestions for Mac and Linux and almost totally clueless for Windows as well. The folks at Stan offer some simple tests that you can run to insure that all the pieces work together.

Stan can easily run in parallel and can take advantage of the multiple cores available on many personal computers.

## Stan, a very simple example

If you ever want to tinker with BUGS, jags, or Stan, you should start with the easiest example possible, a beta-binomial model. The example found on page 3 of Carpenter is just slightly more complicated because it has a vector of Bernoulli observations rather than a single binomial observation.

You need to store the Stan code in a file.

```{r show-beta-bernoulli-code}
f <- "beta-bernoulli.stan"
cat(readLines(f), sep="\n")
```

Although you can specify your data in Stan itself, you would normally find it easier to store your data in a list.

```{r show-beta-bernoulli-data}
N <- 10
y <- c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1)
data_beta_bernoulli <- list(N=N, y=y)
```

Then you tell R where data and the code live 

```{r run-beta-bernoulli-analysis}
library(dplyr)
library(ggplot2)
library(magrittr)
library(rstan)
library(tidyr)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

fit_beta_bernoulli <- stan(file=f,
  data=data_beta_bernoulli, iter= 10000, warmup=1000, chains = 4)
print(fit_beta_bernoulli)
plot(fit_beta_bernoulli)
fit_beta_bernoulli %>% as.data.frame -> sim_beta_bernoulli
ggplot(sim_beta_bernoulli) +
  geom_histogram(aes(x=theta), fill="white", color="black")
```

## Markov Chain Monte Carlo

Stan uses a simulation approach to estimate posterior distributions in a Bayesian model. Recall that a Bayesian model has a prior distribution f($\theta$) and a conditional likelihood g(Y|$\theta$). The posterior distribution is given by the formula

$h(\theta|Y)=\frac{f(\theta)g(Y|\theta)}{\int{f(\theta)g(Y|\theta)d\theta}}$

In many Bayesian applications, $\theta$ is a high dimensional vector. For example, in certain longitudinal and hierarchical models, there is one parameter for each subject in your data set. This means that the integral in the denominator is a high dimensional integral. For some settings, most notably when you have a [conjugate prior distribution](https://en.wikipedia.org/wiki/Conjugate_prior), you can calculate this integral directly. But for many interesting Bayesian data analyses, the integral has no closed form. Researchers in the world of Bayesian data analysis used to have to rely on complex numerical integration approaches.

But in the early 1990s, researchers discovered several simple approaches to simulate posterior distributions. These approaches, under the general name of Markov Chain Monte Carlo (MCMC) methods, relied on the fact that the denominator is just a constant, albeit a very difficult constant to compute. This means that while you might not know the posterior density at any particular value of $\theta_i$, you do know the height of the density at $\theta_i$ relative to the height of the density at $\theta_j$.

One of the simplest MCMC methods, the Metropolis algorithm uses a simple acceptance/rejection method combined with a jumping distribution. You calculate the ratio of density at the point where you currently are and at a point selected by the jumping distribution. If the density is higher at the jumping point, always make the jump. If the density is not higher at the jumping point, stay where you are for another round part of the time and jump part of the time. How often you jump when the density is lower depends on the ratio. This insures that you jump frequently to places where the density is relatively large, but only rarely to places where the density is relatively small (and never to places where the density is zero).

I have a [simple illustration of the Metropolis algorithm](http://www.pmean.com/07/MetropolisAlgorithm.html) on my website. I also developed a simulation of the geometric distribution using the Metropolis algorithm and couched it in terms of a [baby learning how to walk](http://blog.pmean.com/baby-walk/).

There is a fair amount of autocorrelation in the successive values of the MCMC simulation. This occurs for two reasons. First, the jumping process insures that the successive values are close to one another. Second, the acceptance/rejection approach leads to situations where the sequence sometimes "stutters" and stays at the same position. This leads to a patchiness in the simulation that is only overcome with simulation of thousands or tens of thousands of steps.

## Early programs that used MCMC

One of the first programs that allowed amateurs like me to apply MCMC simulations to Bayesian models was WinBUGS. BUGS stands for Bayes Using Gibbs Sampling. Gibbs sampling is another MCMC approach. WinBUGS first appeared on the scene in 1991.

Closely related to WinBUGS is OpenBUGS, an attempt to make the Gibbs sampler available to operating systems other than Windows.

I found that I had better luck using a program called jags (just another Gibbs sampler). It wasn't too picky about what version of R you were using and it had better error messages.

All three programs allow you to set up your data sets in R and then pass your data to that program and then analyze the results within R. These programs allow you to explore non-conjugate priors, which greatly broadens the scope of problems that you can apply Bayesian models to.

## How Stan improves on jags and BUGS

On page 2 of Carpenter et al (2017), Stan differes from BUGS and jags in two ways: First, stan uses a "new imperative probabilistic programming" approach compared to the "declarative graphical modeling" approach of BUGS and jags. Second, Stan uses a new approach to simulate a posterior distribution called the Hamiltonian Markov Chain (HMC). Let me tackle the second issue first.

## Stan and the HMC

The HMC relies on the fact that not only do you know the relative height of the density at $\theta_i$ but you also know the relative gradient. The HMC adds a momentum parameter to the jumping distribution. It's like giving a shove to a hockey puck in a random direction on a curved surface, and the shove causes the puck to travel in a curved path with the curve bending away from areas of low probability and bending towards areas of high probability. Here's [an illustration of HMC in action](https://www.youtube.com/watch?v=59vKonIy2uU) and a [second illustration](http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html).

The use of HMC and an extension called the No U Turns sampler (NUTS) in Stan lets you get a much improved (faster and better coverage) simulation. This in turn allows you to fit Bayesian models to larger and more complex problems.

## Stan and the imperative probabilistc programming approach.

I like Stan, however, not for its speed, but for the more logical layout. Both BUGS and jags create a [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph), and the order of the statements in BUGS and jags is not important.

In Stan, however, order is important and different types of programming statements belong in different parts of your program.

All variables in Stan are static (Carpenter, page 13), meaning that they can't change from integer to real or from vector to matrix in the middle of the program.

You need to be careful when you mix integer and real values. Usually it works, but there are a few exceptions.

You can store more than one integer or real in a (column) vector, a row vector, a matrix, or an array. Use square brackets for these.

Stan allows you to place bounds on individual variables (minimum, maximum, or both) and on vectors (Carpenter, page 14). You can also bound vectors to a unit simplex, unit length, ordered, or positive ordered.

You can constrain matrices to match the same constraints that apply to covariance matrices, correlation matrices, and precision matrices.

## Program blocks

Stan has six program blocks: data, transformed data, parameters, transformed parameters, model, and generated quantities.

## Data block

The data block tells Stan what the structure of the data looks like. Here's an example of a data block.

```{r show-data-block, echo=FALSE}
f <- "~/stan-tutorial/src/hierarchical-binomial.stan"
stan_code <- readLines(f)
start_data_block <- 
data_block <-
  grep("^data", stan_code):(grep("^parameters", stan_code)-1)
cat(stan_code[data_block], sep="\n")
```

## Transformed data block

This block is optional. It is useful for defining constants, and computing transformations of data values within Stan. If you are running Stan within R, it may be just as easy to add the transformed data to the list that you pass to Stan.

The particular example I am using does not have a transformed data block.

## Parameter block

The parameter block defines the structure of parameters in your model. It is very important to put appropriate constraints on parameters here. Stan uses transformations to enforce these constraints. If your parameter is non-negative, Stan will run the HMC on the logarithm of the parameter.

Here is the parameters block for the hierarchical binomial model.

```{r show-parameters-block, echo=FALSE}
parameter_block <-
   grep("^parameters", stan_code):
  (grep("^transformed parameters", stan_code)-1)
cat(stan_code[parameter_block], sep="\n")
```

## Transformed parameter block

This block is optional. It allows you to re-express parameters in a different form. For example, if you have two parameters, alpha and beta, for a beta distribution, you might monitor a transformed parameter mu equal to alpha/(alpha+beta). If you create a transformed parameter, and that parameter is assigned a distribution in the model block, then you need to calculate the Jacobian of the transformation and account for that explicitly. See Carpenter, pages 18-19 for an example.

The rule of thumb is that transformed parameters that appear on the left hand side of the tilde require a Jacobian adjustment, but transformed parameters that appear on the right hand side of the tilde do not require a Jacobian adjustment.

Here is the transformed parameters block for the hierarchical binomial model.

```{r show-transformed-parameters-block, echo=FALSE}
transformed_parameter_block <-
   grep("^transformed parameters", stan_code):
  (grep("^model", stan_code)-1)
cat(stan_code[transformed_parameter_block], sep="\n")
```

## Model block

The model block is where you assign distribution to your parameters (your prior distributions) and to your data (your conditional likelihood).

If you do not specify a prior distribution, Stan generates uniform random variables for you. This would be an improper uniform prior from minus infinity to plus infinity for unconstrained parameters. Depending on the constraint, the uniform prior might be proper or improper.

Neither BUGs nor jags allows improper priors. Stan is okay with improper priors unless they lead to improper posteriors. Be very cautious about improper priors, of course.

Here is the model block for the hierarchical binomial model.

```{r show-model-block, echo=FALSE}
model_block <-
   grep("^model", stan_code):
  (grep("^generated quantities", stan_code)-1)
cat(stan_code[model_block], sep="\n")
```

## Generated quantities block

This block is optional. It allows you to compute posterior predictive values, among other things.

Here is the generated quantities block from the hierarchical binomial model.

```{r show-generated-quantities-block, echo=FALSE}
generated_quantities_block <-
   grep("^generated quantities", stan_code):
   length(stan_code)
cat(stan_code[generated_quantities_block], sep="\n")
```

There are 71 separate studies and you might be interested in how the estimated success rate in each individual study compares to all the other studies. The code in this block calculates the how often the estimated success rate in each individual study is above the average success rate of all the studies, the rank of the success rate in each individual study, and how ofthe the estimated success rate for each individual study is highest among all the studies.

This is an excellent example of how institutional report cards could be compared.

## Running the hierarchical binomial model

The data for the hierarchical binomial model is on the Internet, but you have to skip the first three lines of the file. Also, the file is imported, by default, as a data frame. Stan is expecting data in a list instead. Note also the inconsistent capitalization of N/n that needs to be fixed.

```{r show-rats-list}
f <- "http://www.stat.columbia.edu/~gelman/book/data/rats.asc"
rats_data <- read.table(file=f, header=TRUE, skip=3)
rats_list <- list(
  y=rats_data$y, 
  n=rats_data$N,
  J=length(rats_data$y))
print(rats_list)
rats_data %>%
  mutate(p=y/N) %>%
  mutate(study=factor(row_number())) %>%
  ggplot(aes(study, p)) +
    geom_point() +
    coord_flip()
```

You've seen the individual pieces of the Stan code, but here is everything in one spot.

```{r show-rats-code, echo=FALSE}
f <- "~/stan-tutorial/src/hierarchical-binomial.stan"
cat(readLines(f), sep="\n")
```

The analysis is a lot messier, but the call to Stan from within R is pretty much the same. You just tell Stan where the data lives and where the code lives.

```{r run-rats-analysis}
rats_fit <- stan(file=f,
  data=rats_list,
  iter= 10000, warmup=1000, chains = 4)
```

The output from Stan is quite long and messy.

```{r print-rats-analysis}
print(rats_fit)
```

Some graphs might help.

```{r plot-rats-analysis}
rats_sim <- as.data.frame(rats_fit)
rats_sim %>%
  select(starts_with("theta")) %>%
  gather(study, theta) -> df
df$study %<>%
  sub("theta\\[", "", .) %>%
  sub("\\]", "", .) %>%
  as.numeric %>%
  factor
ggplot(df, aes(x=study, y=theta)) +
  geom_boxplot() +
  coord_flip()
```

You can see in the graph above that some studies have a success rate around 5% and some have success rates as high as 20%.

```{r more-plots}
rats_sim %>%
  select(starts_with("above_avg")) %>%
  gather(study, above_avg)  %>%
  group_by(study) %>%
  summarize(prob_above_avg=mean(above_avg)) %>%
  ungroup -> df
df$study %<>%
  sub("above_avg\\[", "", .) %>%
  sub("\\]", "", .) %>%
  as.numeric
ggplot(df, aes(x=study, y=prob_above_avg)) +
  geom_point() +
  coord_flip()

rats_sim %>%
  select(starts_with("highest")) %>%
  gather(study, highest)  %>%
  group_by(study) %>%
  summarize(prob_highest=mean(highest)) %>%
  ungroup -> df
df$study %<>%
  sub("highest\\[", "", .) %>%
  sub("\\]", "", .) %>%
  as.numeric
ggplot(df, aes(x=study, y=prob_highest)) +
  geom_point() +
  coord_flip()

rats_sim %>%
  select(starts_with("rnk")) %>%
  gather(study, rnk) %>%
  filter(study=="rnk[1]") -> df
ggplot(df, aes(x=rnk)) + 
  geom_bar(aes(y = (..count..)/sum(..count..)),
           color="black", fill="white")
```

Save everything for possible later re-use.

```{r save-everything}
save.image("~/stan-tutorial/data/image.RData")
```
